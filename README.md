# Lexical analysis

Lexical tokenization is conversion of a text into meaningful lexical tokens belonging to categories defined by a "lexer" program.


